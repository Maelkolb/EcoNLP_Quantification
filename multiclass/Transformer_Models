import pandas as pd
import numpy as np
import re
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import classification_report, f1_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset

def load_data(file_path, delimiter, label_col, text_col):
    df = pd.read_csv(file_path, delimiter=delimiter, header=0)
    df = df.dropna(subset=[text_col])
    df[text_col] = df[text_col].astype(str).apply(lambda x: re.sub(r'[^\w\s]', '', x))
    return df

def prepare_labels(df, label_col):
    labels = sorted(list(set(df[label_col].astype(str))))
    label_to_id = {l: i for i, l in enumerate(labels)}
    id_to_label = {v: k for k, v in label_to_id.items()}
    df[label_col] = df[label_col].astype(str).apply(lambda x: label_to_id[x])
    return label_to_id, id_to_label, len(labels)

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        enc = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length)
        item = {key: torch.tensor(val) for key, val in enc.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

def train_and_evaluate_model(df, tokenizer, label_to_id, id_to_label, num_labels, label_col, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    f1_micro_scores, f1_macro_scores, f1_weighted_scores = [], [], []
    for fold, (train_index, test_index) in enumerate(kf.split(df), 1):
        train_df = df.iloc[train_index].reset_index(drop=True)
        test_df = df.iloc[test_index].reset_index(drop=True)
        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)
        train_dataset = TextDataset(train_df["text"].tolist(), train_df[label_col].tolist(), tokenizer)
        val_dataset = TextDataset(val_df["text"].tolist(), val_df[label_col].tolist(), tokenizer)
        test_dataset = TextDataset(test_df["text"].tolist(), test_df[label_col].tolist(), tokenizer)
        model = AutoModelForSequenceClassification.from_pretrained("sentence-transformers/LaBSE", num_labels=num_labels)
        training_args = TrainingArguments(
            output_dir="./results",
            evaluation_strategy="epoch",
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            num_train_epochs=5,
            weight_decay=0.01,
            save_strategy="no"
        )
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer
        )
        trainer.train()
        predictions = trainer.predict(test_dataset)
        preds = predictions.predictions.argmax(axis=1)
        print("Classification Report for fold", fold, ":")
        print(
            classification_report(
                test_df[label_col],
                preds,
                labels=range(num_labels),
                target_names=[id_to_label[i] for i in range(num_labels)]
            )
        )
        f1_micro_scores.append(f1_score(test_df[label_col], preds, average='micro'))
        f1_macro_scores.append(f1_score(test_df[label_col], preds, average='macro'))
        f1_weighted_scores.append(f1_score(test_df[label_col], preds, average='weighted'))
    print("Mean F1-micro:", np.mean(f1_micro_scores))
    print("Mean F1-macro:", np.mean(f1_macro_scores))
    print("Mean F1-weighted:", np.mean(f1_weighted_scores))

if __name__ == "__main__":
    data_file = "frequency_classes_texts.csv"
    delimiter = "@"
    text_col = "text"
    label_col = "label_mr"
    df = load_data(data_file, delimiter, label_col, text_col)
    label_to_id, id_to_label, num_labels = prepare_labels(df, label_col)
    tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/LaBSE")
    train_and_evaluate_model(df, tokenizer, label_to_id, id_to_label, num_labels, label_col)
    '''
    Adjust the model:
    - sentence-transformers/LaBSE
    - bert-base-german-cased
    - deepset/gbert-base
    ...
